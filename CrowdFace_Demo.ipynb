{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdFace Demo: Neural-Adaptive Crowd Segmentation with Contextual Pixel-Space Advertisement Integration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/USERNAME/crowdface/blob/main/CrowdFace_Demo.ipynb)\n",
    "\n",
    "This notebook demonstrates the CrowdFace system, which combines state-of-the-art segmentation models with contextual advertisement placement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision opencv-python transformers diffusers accelerate safetensors huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Bagel repository\n",
    "!git clone https://github.com/ByteDance-Seed/Bagel.git\n",
    "# Add Bagel to the Python path\n",
    "import sys\n",
    "sys.path.append('Bagel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# For Google Colab, we'll use the secrets module to securely handle API keys\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
    "    print(\"Running in Google Colab with secure API key handling\")\n",
    "except:\n",
    "    HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')\n",
    "    print(\"Running in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "### 1. Load SAM2 (Segment Anything Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "# Load SAM2 model and processor\n",
    "model_id = \"facebook/sam2\"\n",
    "sam_processor = SamProcessor.from_pretrained(model_id, token=HUGGINGFACE_TOKEN)\n",
    "sam_model = SamModel.from_pretrained(model_id, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam_model = sam_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load RVM (Robust Video Matting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone RVM repository if not already present\n",
    "!git clone https://github.com/PeterL1n/RobustVideoMatting.git\n",
    "sys.path.append('RobustVideoMatting')\n",
    "\n",
    "try:\n",
    "    from model import MattingNetwork\n",
    "    \n",
    "    # Load RVM model\n",
    "    rvm_model = MattingNetwork('mobilenetv3').eval().to(device)\n",
    "    \n",
    "    # Download RVM weights\n",
    "    if not os.path.exists('rvm_mobilenetv3.pth'):\n",
    "        !wget https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth\n",
    "    \n",
    "    # Load weights\n",
    "    rvm_model.load_state_dict(torch.load('rvm_mobilenetv3.pth', map_location=device))\n",
    "    print(\"RVM model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading RVM model: {e}\")\n",
    "    print(\"Will use fallback methods for matting\")\n",
    "    rvm_model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load BAGEL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up BAGEL model\n",
    "save_dir = \"models/BAGEL-7B-MoT\"\n",
    "repo_id = \"ByteDance-Seed/BAGEL-7B-MoT\"\n",
    "cache_dir = save_dir + \"/cache\"\n",
    "\n",
    "try:\n",
    "    print(\"Downloading BAGEL model (this may take some time)...\")\n",
    "    snapshot_download(cache_dir=cache_dir,\n",
    "      local_dir=save_dir,\n",
    "      repo_id=repo_id,\n",
    "      local_dir_use_symlinks=False,\n",
    "      resume_download=True,\n",
    "      token=HUGGINGFACE_TOKEN,\n",
    "      allow_patterns=[\"*.json\", \"*.safetensors\", \"*.bin\", \"*.py\", \"*.md\", \"*.txt\"],\n",
    "    )\n",
    "    print(\"BAGEL model downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading BAGEL model: {e}\")\n",
    "    print(\"Will use fallback methods for scene understanding and ad placement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BAGEL model\n",
    "try:\n",
    "    from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "    from Bagel.data.transforms import ImageTransform\n",
    "    from Bagel.data.data_utils import add_special_tokens\n",
    "    from Bagel.modeling.bagel import (\n",
    "        BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n",
    "    )\n",
    "    from Bagel.modeling.qwen2 import Qwen2Tokenizer\n",
    "    from Bagel.modeling.bagel.qwen2_navit import NaiveCache\n",
    "    from Bagel.modeling.autoencoder import load_ae\n",
    "    from Bagel.inferencer import InterleaveInferencer\n",
    "    \n",
    "    model_path = save_dir\n",
    "    \n",
    "    # LLM config preparing\n",
    "    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "    llm_config.qk_norm = True\n",
    "    llm_config.tie_word_embeddings = False\n",
    "    llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "    \n",
    "    # ViT config preparing\n",
    "    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "    vit_config.rope = False\n",
    "    vit_config.num_hidden_layers = vit_config.num_hidden_layers - 1\n",
    "    \n",
    "    # VAE loading\n",
    "    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "    \n",
    "    # Bagel config preparing\n",
    "    config = BagelConfig(\n",
    "        visual_gen=True,\n",
    "        visual_und=True,\n",
    "        llm_config=llm_config, \n",
    "        vit_config=vit_config,\n",
    "        vae_config=vae_config,\n",
    "        vit_max_num_patch_per_side=70,\n",
    "        connector_act='gelu_pytorch_tanh',\n",
    "        latent_patch_size=2,\n",
    "        max_latent_size=64,\n",
    "    )\n",
    "    \n",
    "    # Initialize model with empty weights\n",
    "    with init_empty_weights():\n",
    "        language_model = Qwen2ForCausalLM(llm_config)\n",
    "        vit_model = SiglipVisionModel(vit_config)\n",
    "        model = Bagel(language_model, vit_model, config)\n",
    "        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "    \n",
    "    # Load tokenizer and add special tokens\n",
    "    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "    \n",
    "    # Set up transforms\n",
    "    vae_transform = ImageTransform(1024, 512, 16)\n",
    "    vit_transform = ImageTransform(980, 224, 14)\n",
    "    \n",
    "    # Set up device map for model loading\n",
    "    device_map = infer_auto_device_map(\n",
    "        model,\n",
    "        max_memory={i: \"80GiB\" for i in range(torch.cuda.device_count())},\n",
    "        no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
    "    )\n",
    "    \n",
    "    # Define modules that should be on the same device\n",
    "    same_device_modules = [\n",
    "        'language_model.model.embed_tokens',\n",
    "        'time_embedder',\n",
    "        'latent_pos_embed',\n",
    "        'vae2llm',\n",
    "        'llm2vae',\n",
    "        'connector',\n",
    "    ]\n",
    "    \n",
    "    # Load model weights\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model, \n",
    "        os.path.join(model_path, \"pytorch_model.bin\"),\n",
    "        device_map=device_map,\n",
    "        offload_folder=None,\n",
    "        offload_state_dict=False,\n",
    "        same_device_modules=same_device_modules,\n",
    "    )\n",
    "    \n",
    "    # Initialize the inferencer\n",
    "    bagel_inferencer = InterleaveInferencer(\n",
    "        model=model, \n",
    "        vae_model=vae_model, \n",
    "        tokenizer=tokenizer, \n",
    "        vae_transform=vae_transform, \n",
    "        vit_transform=vit_transform, \n",
    "        new_token_ids=new_token_ids\n",
    "    )\n",
    "    \n",
    "    print(\"BAGEL model initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing BAGEL model: {e}\")\n",
    "    print(\"Will use fallback methods for scene understanding and ad placement.\")\n",
    "    bagel_inferencer = None"
   ]
  }
]
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrowdFace Pipeline Implementation\n",
    "\n",
    "Now let's implement the core components of our CrowdFace pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAGELSceneUnderstanding:\n",
    "    def __init__(self, bagel_inferencer):\n",
    "        self.bagel_inferencer = bagel_inferencer\n",
    "        self.scene_cache = {}  # Cache scene analysis to avoid redundant processing\n",
    "        \n",
    "    def analyze_frame(self, frame, frame_idx):\n",
    "        \"\"\"Analyze frame using BAGEL to extract scene context and objects\"\"\"\n",
    "        # Check cache first to avoid redundant processing\n",
    "        if frame_idx in self.scene_cache:\n",
    "            return self.scene_cache[frame_idx]\n",
    "            \n",
    "        # Convert frame to PIL Image for BAGEL\n",
    "        if isinstance(frame, np.ndarray):\n",
    "            pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            pil_frame = frame\n",
    "            \n",
    "        # Use BAGEL for scene understanding if available\n",
    "        if self.bagel_inferencer is not None:\n",
    "            try:\n",
    "                # Use BAGEL for scene understanding\n",
    "                prompt = \"Describe this scene in detail, focusing on the environment, people, objects, and overall context.\"\n",
    "                \n",
    "                # Call the BAGEL inferencer with the image and prompt\n",
    "                output_dict = self.bagel_inferencer(\n",
    "                    image=pil_frame,\n",
    "                    text=prompt,\n",
    "                    think=True,  # Enable thinking mode for detailed analysis\n",
    "                    cfg_text_scale=4.0,  # Control text guidance\n",
    "                    num_timesteps=20,    # Faster inference for analysis\n",
    "                )\n",
    "                \n",
    "                scene_analysis = output_dict['text']\n",
    "                \n",
    "                # Extract key information from BAGEL's analysis\n",
    "                scene_info = {\n",
    "                    'description': scene_analysis,\n",
    "                    'context': self._extract_context(scene_analysis),\n",
    "                    'objects': self._extract_objects(scene_analysis),\n",
    "                    'mood': self._extract_mood(scene_analysis),\n",
    "                    'suitable_ad_types': self._recommend_ad_types(scene_analysis)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in BAGEL scene analysis: {e}\")\n",
    "                scene_info = self._fallback_analysis(frame)\n",
    "        else:\n",
    "            # Fallback to basic analysis if BAGEL is not available\n",
    "            scene_info = self._fallback_analysis(frame)\n",
    "            \n",
    "        # Cache the results\n",
    "        self.scene_cache[frame_idx] = scene_info\n",
    "        return scene_info\n",
    "    \n",
    "    def _extract_context(self, analysis):\n",
    "        \"\"\"Extract context keywords from the scene analysis\"\"\"\n",
    "        # Simple keyword extraction - in a real system this would be more sophisticated\n",
    "        keywords = []\n",
    "        if \"outdoor\" in analysis.lower():\n",
    "            keywords.append(\"outdoor\")\n",
    "        if \"indoor\" in analysis.lower():\n",
    "            keywords.append(\"indoor\")\n",
    "        if \"crowd\" in analysis.lower():\n",
    "            keywords.append(\"crowded\")\n",
    "        if \"urban\" in analysis.lower():\n",
    "            keywords.append(\"urban\")\n",
    "        if \"rural\" in analysis.lower():\n",
    "            keywords.append(\"rural\")\n",
    "        if \"sports\" in analysis.lower() or \"game\" in analysis.lower():\n",
    "            keywords.append(\"sports\")\n",
    "        if \"concert\" in analysis.lower() or \"music\" in analysis.lower():\n",
    "            keywords.append(\"entertainment\")\n",
    "        \n",
    "        return keywords if keywords else [\"general\"]\n",
    "    \n",
    "    def _extract_objects(self, analysis):\n",
    "        \"\"\"Extract objects mentioned in the scene analysis\"\"\"\n",
    "        objects = []\n",
    "        common_objects = [\"person\", \"people\", \"car\", \"building\", \"tree\", \"sign\", \"phone\", \"computer\"]\n",
    "        \n",
    "        for obj in common_objects:\n",
    "            if obj in analysis.lower():\n",
    "                objects.append(obj)\n",
    "                \n",
    "        return objects\n",
    "    \n",
    "    def _extract_mood(self, analysis):\n",
    "        \"\"\"Extract mood/atmosphere from the scene analysis\"\"\"\n",
    "        moods = []\n",
    "        mood_keywords = {\n",
    "            \"happy\": [\"happy\", \"cheerful\", \"joyful\", \"excited\"],\n",
    "            \"serious\": [\"serious\", \"formal\", \"professional\"],\n",
    "            \"relaxed\": [\"relaxed\", \"calm\", \"peaceful\"],\n",
    "            \"energetic\": [\"energetic\", \"dynamic\", \"active\"],\n",
    "            \"tense\": [\"tense\", \"anxious\", \"stressed\"]\n",
    "        }\n",
    "        \n",
    "        for mood, keywords in mood_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in analysis.lower():\n",
    "                    moods.append(mood)\n",
    "                    break\n",
    "                    \n",
    "        return moods if moods else [\"neutral\"]\n",
    "    \n",
    "    def _recommend_ad_types(self, analysis):\n",
    "        \"\"\"Recommend suitable ad types based on scene analysis\"\"\"\n",
    "        ad_types = []\n",
    "        \n",
    "        # Context-based recommendations\n",
    "        if any(kw in analysis.lower() for kw in [\"sports\", \"game\", \"athletic\", \"fitness\"]):\n",
    "            ad_types.append(\"sports_equipment\")\n",
    "            ad_types.append(\"energy_drinks\")\n",
    "            \n",
    "        if any(kw in analysis.lower() for kw in [\"food\", \"restaurant\", \"eating\", \"dining\"]):\n",
    "            ad_types.append(\"food_delivery\")\n",
    "            ad_types.append(\"restaurants\")\n",
    "            \n",
    "        if any(kw in analysis.lower() for kw in [\"outdoor\", \"nature\", \"park\"]):\n",
    "            ad_types.append(\"outdoor_gear\")\n",
    "            ad_types.append(\"travel\")\n",
    "            \n",
    "        if any(kw in analysis.lower() for kw in [\"technology\", \"phone\", \"computer\", \"digital\"]):\n",
    "            ad_types.append(\"electronics\")\n",
    "            ad_types.append(\"software\")\n",
    "            \n",
    "        # Default to general ads if no specific context is detected\n",
    "        if not ad_types:\n",
    "            ad_types = [\"general_retail\", \"services\"]\n",
    "            \n",
    "        return ad_types\n",
    "    \n",
    "    def _fallback_analysis(self, frame):\n",
    "        \"\"\"Perform basic analysis when BAGEL is not available\"\"\"\n",
    "        # Simple fallback that provides basic information\n",
    "        return {\n",
    "            'description': \"A scene with potential crowd elements.\",\n",
    "            'context': [\"general\"],\n",
    "            'objects': [\"person\"],\n",
    "            'mood': [\"neutral\"],\n",
    "            'suitable_ad_types': [\"general_retail\", \"services\"]\n",
    "        }"
   ]
  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAGELAdPlacement:\n",
    "    def __init__(self, bagel_inferencer):\n",
    "        self.bagel_inferencer = bagel_inferencer\n",
    "        \n",
    "    def find_optimal_placement(self, frame, mask, scene_info=None):\n",
    "        \"\"\"Find optimal ad placement using BAGEL's scene understanding\"\"\"\n",
    "        # Convert mask to binary\n",
    "        binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # If no contours found, return None\n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        # Find the largest contour (assuming it's a person)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        if cv2.contourArea(largest_contour) < 1000:  # Minimum size threshold\n",
    "            return None\n",
    "        \n",
    "        # Get bounding box\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Use BAGEL for intelligent placement if available\n",
    "        if self.bagel_inferencer is not None and scene_info is not None:\n",
    "            try:\n",
    "                # Convert frame to PIL Image for BAGEL\n",
    "                if isinstance(frame, np.ndarray):\n",
    "                    pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                else:\n",
    "                    pil_frame = frame\n",
    "                    \n",
    "                # Generate a prompt for BAGEL to analyze optimal ad placement\n",
    "                context_str = \", \".join(scene_info['context']) if 'context' in scene_info else \"general\"\n",
    "                prompt = f\"Where would be the best place to put an advertisement in this {context_str} scene? Answer with one of: right, left, top, bottom.\"\n",
    "                \n",
    "                # Use BAGEL to analyze the frame for optimal placement\n",
    "                output_dict = self.bagel_inferencer(\n",
    "                    image=pil_frame,\n",
    "                    text=prompt,\n",
    "                    cfg_text_scale=3.0,\n",
    "                    num_timesteps=20,\n",
    "                )\n",
    "                \n",
    "                placement_analysis = output_dict['text']\n",
    "                \n",
    "                # Parse the placement recommendation\n",
    "                if \"right\" in placement_analysis.lower():\n",
    "                    ad_x = x + w + 20  # Place to the right\n",
    "                    ad_y = y\n",
    "                elif \"left\" in placement_analysis.lower():\n",
    "                    ad_x = max(0, x - 100)  # Place to the left\n",
    "                    ad_y = y\n",
    "                elif \"top\" in placement_analysis.lower():\n",
    "                    ad_x = x\n",
    "                    ad_y = max(0, y - 100)  # Place above\n",
    "                elif \"bottom\" in placement_analysis.lower():\n",
    "                    ad_x = x\n",
    "                    ad_y = y + h + 20  # Place below\n",
    "                else:\n",
    "                    # Default to right side placement\n",
    "                    ad_x = x + w + 20\n",
    "                    ad_y = y\n",
    "                    \n",
    "                return (ad_x, ad_y)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in BAGEL ad placement: {e}\")\n",
    "                # Fall back to default placement\n",
    "        \n",
    "        # Default placement (to the right of the person)\n",
    "        ad_x = x + w + 20\n",
    "        ad_y = y\n",
    "        \n",
    "        return (ad_x, ad_y)"
   ]
  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAGELAdOptimization:\n",
    "    def __init__(self, bagel_inferencer):\n",
    "        self.bagel_inferencer = bagel_inferencer\n",
    "        self.ad_cache = {}  # Cache generated ads\n",
    "        \n",
    "    def optimize_ad_for_scene(self, ad_image, scene_info, ad_type=None):\n",
    "        \"\"\"Optimize ad content for the specific scene context\"\"\"\n",
    "        # If BAGEL is not available, return original ad\n",
    "        if self.bagel_inferencer is None:\n",
    "            return ad_image\n",
    "            \n",
    "        # Generate cache key\n",
    "        context_str = \",\".join(scene_info['context']) if 'context' in scene_info else \"general\"\n",
    "        mood_str = \",\".join(scene_info['mood']) if 'mood' in scene_info else \"neutral\"\n",
    "        cache_key = f\"{ad_type}_{context_str}_{mood_str}\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_key in self.ad_cache:\n",
    "            return self.ad_cache[cache_key]\n",
    "            \n",
    "        try:\n",
    "            # Convert ad_image to PIL Image if it's not already\n",
    "            if isinstance(ad_image, np.ndarray):\n",
    "                pil_ad = Image.fromarray(cv2.cvtColor(ad_image, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                pil_ad = ad_image\n",
    "                \n",
    "            # Create a prompt for BAGEL to optimize the ad\n",
    "            prompt = f\"Modify this advertisement to better match a {mood_str} scene with {context_str} context.\"\n",
    "            \n",
    "            # Use BAGEL to generate an optimized ad\n",
    "            output_dict = self.bagel_inferencer(\n",
    "                image=pil_ad,\n",
    "                text=prompt,\n",
    "                cfg_text_scale=6.0,\n",
    "                cfg_img_scale=1.5,\n",
    "                num_timesteps=50,\n",
    "            )\n",
    "            \n",
    "            optimized_ad = output_dict['image']\n",
    "            \n",
    "            # Cache the result\n",
    "            self.ad_cache[cache_key] = optimized_ad\n",
    "            return optimized_ad\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in BAGEL ad optimization: {e}\")\n",
    "            # Fall back to original ad\n",
    "            return ad_image"
   ]
  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdFacePipeline:\n",
    "    def __init__(self, sam_model, sam_processor, rvm_model, bagel_inferencer=None):\n",
    "        self.sam_model = sam_model\n",
    "        self.sam_processor = sam_processor\n",
    "        self.rvm_model = rvm_model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize BAGEL-enhanced components if BAGEL model is available\n",
    "        if bagel_inferencer is not None:\n",
    "            self.scene_analyzer = BAGELSceneUnderstanding(bagel_inferencer)\n",
    "            self.ad_placer = BAGELAdPlacement(bagel_inferencer)\n",
    "            self.ad_optimizer = BAGELAdOptimization(bagel_inferencer)\n",
    "        else:\n",
    "            self.scene_analyzer = None\n",
    "            self.ad_placer = None\n",
    "            self.ad_optimizer = None\n",
    "        \n",
    "    def segment_people(self, frame):\n",
    "        \"\"\"Segment people in the frame using SAM2\"\"\"\n",
    "        # Convert frame to RGB if it's in BGR format\n",
    "        if isinstance(frame, np.ndarray) and frame.shape[2] == 3:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            rgb_frame = frame\n",
    "            \n",
    "        # Process the image with SAM\n",
    "        inputs = self.sam_processor(rgb_frame, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate automatic mask predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sam_model(**inputs)\n",
    "            \n",
    "        # Get the predicted masks\n",
    "        masks = self.sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(),\n",
    "            inputs[\"original_sizes\"].cpu(),\n",
    "            inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )\n",
    "        \n",
    "        # Filter masks to only include people (this is a simplified approach)\n",
    "        # In a real implementation, you would use a classifier to identify people\n",
    "        combined_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Take the largest mask as a person (simplified approach)\n",
    "        if len(masks) > 0 and len(masks[0]) > 0:\n",
    "            largest_mask = None\n",
    "            largest_area = 0\n",
    "            \n",
    "            for mask in masks[0]:\n",
    "                mask_np = mask.numpy()\n",
    "                area = np.sum(mask_np)\n",
    "                if area > largest_area:\n",
    "                    largest_area = area\n",
    "                    largest_mask = mask_np\n",
    "                    \n",
    "            if largest_mask is not None:\n",
    "                combined_mask = largest_mask.astype(np.uint8) * 255\n",
    "        \n",
    "        return combined_mask\n",
    "        \n",
    "    def generate_matte(self, frame, prev_frame=None, prev_fgr=None, prev_pha=None, prev_state=None):\n",
    "        \"\"\"Generate alpha matte using RVM\"\"\"\n",
    "        if self.rvm_model is None:\n",
    "            # Fallback to simple mask if RVM is not available\n",
    "            return self.segment_people(frame)\n",
    "            \n",
    "        try:\n",
    "            # Convert frame to tensor\n",
    "            frame_tensor = torch.from_numpy(frame).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "            frame_tensor = frame_tensor.to(self.device)\n",
    "            \n",
    "            # Initialize previous frame and state if not provided\n",
    "            if prev_frame is None:\n",
    "                prev_frame = torch.zeros_like(frame_tensor)\n",
    "            if prev_fgr is None:\n",
    "                prev_fgr = torch.zeros_like(frame_tensor)\n",
    "            if prev_pha is None:\n",
    "                prev_pha = torch.zeros((1, 1, frame.shape[0], frame.shape[1]), device=self.device)\n",
    "            if prev_state is None:\n",
    "                prev_state = None\n",
    "                \n",
    "            # Generate matte\n",
    "            with torch.no_grad():\n",
    "                fgr, pha, state = self.rvm_model(frame_tensor, prev_frame, prev_fgr, prev_pha, prev_state)\n",
    "                \n",
    "            # Convert alpha matte to numpy array\n",
    "            alpha_matte = pha[0, 0].cpu().numpy() * 255\n",
    "            alpha_matte = alpha_matte.astype(np.uint8)\n",
    "            \n",
    "            return alpha_matte, fgr, pha, state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in RVM matting: {e}\")\n",
    "            # Fallback to segmentation mask\n",
    "            return self.segment_people(frame), None, None, None\n",
    "        \n",
    "    def find_ad_placement(self, frame, mask, scene_info=None):\n",
    "        \"\"\"Find suitable locations for ad placement\"\"\"\n",
    "        # Use BAGEL-enhanced ad placement if available\n",
    "        if self.ad_placer is not None and scene_info is not None:\n",
    "            return self.ad_placer.find_optimal_placement(frame, mask, scene_info)\n",
    "        \n",
    "        # Fall back to original method if BAGEL is not available\n",
    "        binary_mask = (mask > 128).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "            \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Default placement to the right of the person\n",
    "        ad_x = x + w + 20\n",
    "        ad_y = y\n",
    "        \n",
    "        return (ad_x, ad_y)\n",
    "        \n",
    "    def optimize_ad(self, ad_image, scene_info=None):\n",
    "        \"\"\"Optimize ad content based on scene context\"\"\"\n",
    "        if self.ad_optimizer is not None and scene_info is not None:\n",
    "            return self.ad_optimizer.optimize_ad_for_scene(ad_image, scene_info)\n",
    "        return ad_image\n",
    "        \n",
    "    def place_ad(self, frame, ad_image, position, scale=0.5, scene_info=None):\n",
    "        \"\"\"Place the ad in the frame at the specified position\"\"\"\n",
    "        # Optimize ad content if scene info is available\n",
    "        if scene_info is not None and self.ad_optimizer is not None:\n",
    "            ad_image = self.optimize_ad(ad_image, scene_info)\n",
    "            \n",
    "        # Determine optimal scale based on scene context\n",
    "        if scene_info is not None and 'description' in scene_info:\n",
    "            if \"crowded\" in scene_info['description'].lower():\n",
    "                scale = 0.3  # Smaller ads in crowded scenes\n",
    "            elif \"spacious\" in scene_info['description'].lower():\n",
    "                scale = 0.7  # Larger ads in spacious scenes\n",
    "        \n",
    "        # Convert ad_image to numpy array if it's a PIL Image\n",
    "        if isinstance(ad_image, Image.Image):\n",
    "            ad_image = np.array(ad_image)\n",
    "            # Convert RGB to BGR if needed\n",
    "            if ad_image.shape[2] == 3:\n",
    "                ad_image = cv2.cvtColor(ad_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Resize ad image\n",
    "        ad_height = int(frame.shape[0] * scale)\n",
    "        ad_width = int(ad_image.shape[1] * (ad_height / ad_image.shape[0]))\n",
    "        ad_resized = cv2.resize(ad_image, (ad_width, ad_height))\n",
    "        \n",
    "        # Extract position\n",
    "        x, y = position\n",
    "        \n",
    "        # Ensure the ad fits within the frame\n",
    "        if x + ad_width > frame.shape[1]:\n",
    "            x = frame.shape[1] - ad_width\n",
    "        if y + ad_height > frame.shape[0]:\n",
    "            y = frame.shape[0] - ad_height\n",
    "            \n",
    "        # Create a copy of the frame\n",
    "        result = frame.copy()\n",
    "        \n",
    "        # Check if ad has an alpha channel\n",
    "        if ad_resized.shape[2] == 4:\n",
    "            # Extract alpha channel\n",
    "            alpha = ad_resized[:, :, 3] / 255.0\n",
    "            alpha = np.expand_dims(alpha, axis=2)\n",
    "            \n",
    "            # Extract RGB channels\n",
    "            rgb = ad_resized[:, :, :3]\n",
    "            \n",
    "            # Get the region of interest in the frame\n",
    "            roi = result[y:y+ad_height, x:x+ad_width]\n",
    "            \n",
    "            # Blend the ad with the frame using alpha\n",
    "            blended = (1.0 - alpha) * roi + alpha * rgb\n",
    "            \n",
    "            # Place the blended image back into the frame\n",
    "            result[y:y+ad_height, x:x+ad_width] = blended\n",
    "        else:\n",
    "            # Simple overlay without alpha blending\n",
    "            result[y:y+ad_height, x:x+ad_width] = ad_resized\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def process_video(self, video_path, ad_image, output_path, max_frames=None):\n",
    "        \"\"\"Process the entire video with BAGEL-enhanced intelligence\"\"\"\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file {video_path}\")\n",
    "            return\n",
    "            \n",
    "        # Get video properties\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if max_frames is not None:\n",
    "            frame_count = min(frame_count, max_frames)\n",
    "            \n",
    "        # Create video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Load ad image\n",
    "        if isinstance(ad_image, str):\n",
    "            ad_img = cv2.imread(ad_image, cv2.IMREAD_UNCHANGED)\n",
    "        else:\n",
    "            ad_img = ad_image\n",
    "            \n",
    "        # Process each frame\n",
    "        frame_idx = 0\n",
    "        ad_position = None\n",
    "        scene_info = None\n",
    "        prev_fgr, prev_pha, prev_state = None, None, None\n",
    "        \n",
    "        for _ in tqdm(range(frame_count), desc=\"Processing video\"):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Every 30 frames, analyze scene and re-detect people and ad placement\n",
    "            if frame_idx % 30 == 0:\n",
    "                # Analyze scene with BAGEL if available\n",
    "                if self.scene_analyzer is not None:\n",
    "                    scene_info = self.scene_analyzer.analyze_frame(frame, frame_idx)\n",
    "                    print(f\"Scene context: {scene_info['context']}\")\n",
    "                    print(f\"Recommended ad types: {scene_info['suitable_ad_types']}\")\n",
    "                \n",
    "                # Generate matte\n",
    "                if self.rvm_model is not None:\n",
    "                    mask, prev_fgr, prev_pha, prev_state = self.generate_matte(frame)\n",
    "                else:\n",
    "                    mask = self.segment_people(frame)\n",
    "                \n",
    "                # Find ad placement using scene information\n",
    "                ad_position = self.find_ad_placement(frame, mask, scene_info)\n",
    "            \n",
    "            # For frames between key frames, update the matte with RVM\n",
    "            elif self.rvm_model is not None and prev_pha is not None:\n",
    "                prev_frame = torch.from_numpy(frame).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "                prev_frame = prev_frame.to(self.device)\n",
    "                mask, prev_fgr, prev_pha, prev_state = self.generate_matte(\n",
    "                    frame, prev_frame, prev_fgr, prev_pha, prev_state\n",
    "                )\n",
    "            \n",
    "            # If we found a position for the ad, place it\n",
    "            if ad_position is not None:\n",
    "                frame = self.place_ad(frame, ad_img, ad_position, scene_info=scene_info)\n",
    "            \n",
    "            # Write the frame to the output video\n",
    "            out.write(frame)\n",
    "            frame_idx += 1\n",
    "            \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Video processing complete. Output saved to {output_path}\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the CrowdFace Pipeline\n",
    "\n",
    "Now let's initialize our pipeline with all the models we've loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CrowdFace pipeline\n",
    "pipeline = CrowdFacePipeline(\n",
    "    sam_model=sam_model,\n",
    "    sam_processor=sam_processor,\n",
    "    rvm_model=rvm_model if 'rvm_model' in locals() else None,\n",
    "    bagel_inferencer=bagel_inferencer if 'bagel_inferencer' in locals() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Pipeline on a Sample Video\n",
    "\n",
    "Let's download a sample video and test our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample video\n",
    "!wget -O sample_video.mp4 https://pixabay.com/videos/download/video-41758_source.mp4?attachment\n",
    "\n",
    "# Download a sample ad image\n",
    "!wget -O sample_ad.png https://raw.githubusercontent.com/USERNAME/crowdface/main/assets/sample_ad.png\n",
    "\n",
    "# If the ad image doesn't exist, create a simple one\n",
    "import os\n",
    "if not os.path.exists('sample_ad.png'):\n",
    "    # Create a simple ad image\n",
    "    ad_img = np.ones((300, 500, 4), dtype=np.uint8) * 255\n",
    "    # Add some text\n",
    "    cv2.putText(ad_img, \"SAMPLE AD\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255, 255), 5)\n",
    "    cv2.imwrite('sample_ad.png', ad_img)\n",
    "    print(\"Created a sample ad image\")\n",
    "else:\n",
    "    print(\"Using downloaded ad image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the video\n",
    "pipeline.process_video(\n",
    "    video_path='sample_video.mp4',\n",
    "    ad_image='sample_ad.png',\n",
    "    output_path='output_video.mp4',\n",
    "    max_frames=100  # Limit to 100 frames for faster processing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "\n",
    "Let's display a few frames from the output video to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Display the output video\n",
    "Video('output_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the CrowdFace system, which combines:\n",
    "\n",
    "1. **SAM2** for precise crowd segmentation\n",
    "2. **RVM** for high-quality video matting\n",
    "3. **BAGEL** for intelligent scene understanding and ad placement\n",
    "\n",
    "This integration creates a sophisticated product that's difficult to replicate because it combines multiple cutting-edge AI models in a way that enhances each component. The result is a system that not only places ads in videos but does so with an understanding of scene context, optimal placement, and content optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
