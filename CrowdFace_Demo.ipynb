{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdFace: Neural-Adaptive Crowd Segmentation with Contextual Pixel-Space Advertisement Integration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BlackBoyZeus/CrowdFace/blob/main/CrowdFace_Demo.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use CrowdFace for post-production video processing with contextual advertisement integration using GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's set up our environment and install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab, installing dependencies...\")\n",
    "    !pip install torch torchvision opencv-python transformers diffusers huggingface_hub\n",
    "    !git clone https://github.com/BlackBoyZeus/CrowdFace.git\n",
    "    %cd CrowdFace\n",
    "    \n",
    "    # Set up Hugging Face access\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        if hf_token:\n",
    "            print(\"Found Hugging Face token in Colab secrets!\")\n",
    "        else:\n",
    "            hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
    "    except:\n",
    "        print(\"To access gated models, please provide your Hugging Face token when prompted\")\n",
    "        hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"Running locally, make sure you have the required dependencies installed.\")\n",
    "    # For local execution, you can set your HF token here or via environment variables\n",
    "    if \"HUGGING_FACE_HUB_TOKEN\" not in os.environ:\n",
    "        hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability\n",
    "\n",
    "Let's verify that we have GPU access for accelerated processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SAM2 and RVM Models\n",
    "\n",
    "CrowdFace uses SAM2 (Segment Anything Model 2) for segmentation and RVM (Robust Video Matting) for high-quality alpha matte generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up SAM2\n",
    "from transformers import SamModel, SamProcessor\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "if \"HUGGING_FACE_HUB_TOKEN\" in os.environ and os.environ[\"HUGGING_FACE_HUB_TOKEN\"]:\n",
    "    login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "    print(\"Logged in to Hugging Face Hub\")\n",
    "    use_auth = True\n",
    "else:\n",
    "    print(\"No Hugging Face token provided, will try to download models without authentication\")\n",
    "    use_auth = False\n",
    "\n",
    "# Load SAM2 model\n",
    "model_id = \"facebook/sam2-large\"\n",
    "try:\n",
    "    if use_auth:\n",
    "        sam_model = SamModel.from_pretrained(model_id, use_auth_token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        sam_processor = SamProcessor.from_pretrained(model_id, use_auth_token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "    else:\n",
    "        sam_model = SamModel.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        sam_processor = SamProcessor.from_pretrained(model_id)\n",
    "    print(\"SAM2 model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SAM2 model: {e}\")\n",
    "    print(\"Please make sure you have a valid Hugging Face token with access to this model.\")\n",
    "    print(\"You can get a token at https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RVM for video matting\n",
    "# Clone the official RVM repository\n",
    "!git clone https://github.com/PeterL1n/RobustVideoMatting\n",
    "%cd RobustVideoMatting\n",
    "\n",
    "# Download the pre-trained model checkpoint (mobilenetv3)\n",
    "!pip install -q gdown\n",
    "\n",
    "# Try different model checkpoints if one fails\n",
    "# First try the official mobilenetv3 checkpoint\n",
    "!gdown https://drive.google.com/uc?id=1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O rvm_mobilenetv3.pth\n",
    "\n",
    "# Return to the main directory\n",
    "%cd ..\n",
    "\n",
    "# Import the model\n",
    "import sys\n",
    "sys.path.append('RobustVideoMatting')\n",
    "from model import MattingNetwork\n",
    "\n",
    "# Load RVM model with robust error handling\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rvm_model = MattingNetwork('mobilenetv3').to(device)\n",
    "\n",
    "# Try loading with strict=True first, then fall back to strict=False if needed\n",
    "try:\n",
    "    rvm_model.load_state_dict(torch.load(\"RobustVideoMatting/rvm_mobilenetv3.pth\", map_location=device))\n",
    "    print(\"RVM model loaded successfully with strict=True!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading RVM model with strict=True: {e}\")\n",
    "    print(\"Attempting to load with strict=False to handle key mismatches...\")\n",
    "    try:\n",
    "        rvm_model.load_state_dict(torch.load(\"RobustVideoMatting/rvm_mobilenetv3.pth\", map_location=device), strict=False)\n",
    "        print(\"RVM model loaded with strict=False. Some weights may not be loaded correctly.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error loading RVM model even with strict=False: {e2}\")\n",
    "        print(\"Trying alternative model checkpoint...\")\n",
    "        \n",
    "        # Try alternative checkpoint URL if available\n",
    "        !gdown https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth -O RobustVideoMatting/rvm_mobilenetv3_alt.pth\n",
    "        \n",
    "        try:\n",
    "            rvm_model.load_state_dict(torch.load(\"RobustVideoMatting/rvm_mobilenetv3_alt.pth\", map_location=device), strict=False)\n",
    "            print(\"RVM model loaded with alternative checkpoint and strict=False.\")\n",
    "        except Exception as e3:\n",
    "            print(f\"Failed to load RVM model with all attempts: {e3}\")\n",
    "            print(\"Proceeding with uninitialized model. Results may be poor.\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "rvm_model.eval()\n",
    "print(\"RVM model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Test Video\n",
    "\n",
    "Upload a video file to process with CrowdFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Please upload a video file:\")\n",
    "    uploaded = files.upload()\n",
    "    video_path = list(uploaded.keys())[0]\n",
    "else:\n",
    "    # For local testing, specify your video path\n",
    "    video_path = \"path/to/your/video.mp4\"\n",
    "\n",
    "# Display video information\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.release()\n",
    "\n",
    "print(f\"Video loaded: {video_path}\")\n",
    "print(f\"Resolution: {width}x{height}\")\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"Frame count: {frame_count}\")\n",
    "print(f\"Duration: {frame_count/fps:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Advertisement Image\n",
    "\n",
    "Upload an image to be used as an advertisement in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Please upload an advertisement image:\")\n",
    "    uploaded = files.upload()\n",
    "    ad_path = list(uploaded.keys())[0]\n",
    "else:\n",
    "    # For local testing, specify your ad image path\n",
    "    ad_path = \"path/to/your/ad.png\"\n",
    "\n",
    "# Load and display the ad image\n",
    "ad_image = cv2.imread(ad_path)\n",
    "ad_image = cv2.cvtColor(ad_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"Advertisement image loaded: {ad_path}\")\n",
    "print(f\"Image size: {ad_image.shape[1]}x{ad_image.shape[0]}\")\n",
    "display(Image.fromarray(ad_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CrowdFace Pipeline\n",
    "\n",
    "Now let's implement the core functionality of CrowdFace for video processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdFacePipeline:\n",
    "    def __init__(self, sam_model, sam_processor, rvm_model):\n",
    "        self.sam_model = sam_model\n",
    "        self.sam_processor = sam_processor\n",
    "        self.rvm_model = rvm_model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def segment_people(self, frame):\n",
    "        \"\"\"Segment people in the frame using SAM2\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image with SAM\n",
    "        inputs = self.sam_processor(rgb_frame, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate automatic masks\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sam_model(**inputs, multimask_output=True)\n",
    "        \n",
    "        # Get the best mask\n",
    "        masks = self.sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(),\n",
    "            inputs[\"original_sizes\"].cpu(),\n",
    "            inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )\n",
    "        scores = outputs.iou_scores.cpu()\n",
    "        \n",
    "        # Filter masks to only include people (assuming higher scores for people)\n",
    "        best_mask_idx = torch.argmax(scores)\n",
    "        best_mask = masks[0][best_mask_idx].numpy()\n",
    "        \n",
    "        return best_mask\n",
    "    \n",
    "    def generate_matte(self, frame, prev_frame=None, prev_fgr=None, prev_pha=None, prev_state=None):\n",
    "        \"\"\"Generate alpha matte using RVM\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        frame_tensor = torch.from_numpy(rgb_frame).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "        frame_tensor = frame_tensor.to(self.device)\n",
    "        \n",
    "        # Process with RVM - handle different return value formats\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if prev_frame is None:\n",
    "                    # Try the format that returns (fgr, pha, *rec)\n",
    "                    outputs = self.rvm_model(frame_tensor, None, None)\n",
    "                    if isinstance(outputs, tuple) and len(outputs) >= 3:\n",
    "                        fgr, pha, *rec = outputs\n",
    "                        state = rec[0] if rec else None\n",
    "                    else:\n",
    "                        # Handle case where model returns different format\n",
    "                        fgr, pha, state = outputs[0], outputs[1], outputs[2] if len(outputs) > 2 else None\n",
    "                else:\n",
    "                    # Try with previous state\n",
    "                    outputs = self.rvm_model(frame_tensor, prev_frame, prev_state)\n",
    "                    if isinstance(outputs, tuple) and len(outputs) >= 3:\n",
    "                        fgr, pha, *rec = outputs\n",
    "                        state = rec[0] if rec else None\n",
    "                    else:\n",
    "                        # Handle case where model returns different format\n",
    "                        fgr, pha, state = outputs[0], outputs[1], outputs[2] if len(outputs) > 2 else None\n",
    "            except Exception as e:\n",
    "                print(f\"Error in RVM inference: {e}\")\n",
    "                # Provide fallback values if RVM fails\n",
    "                h, w = frame.shape[:2]\n",
    "                fgr = frame_tensor  # Use original frame as foreground\n",
    "                pha = torch.ones((1, 1, h, w), device=self.device)  # Full alpha\n",
    "                state = None\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        alpha_matte = pha[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        foreground = fgr[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        return alpha_matte, foreground, frame_tensor, fgr, pha, state\n",
    "    \n",
    "    def find_ad_placement(self, mask, min_size=1000):\n",
    "        \"\"\"Find suitable locations for ad placement\"\"\"\n",
    "        # Convert mask to binary\n",
    "        binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Find the largest contour (assuming it's a person)\n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        if cv2.contourArea(largest_contour) < min_size:\n",
    "            return None\n",
    "        \n",
    "        # Get bounding box\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Find a suitable location near the person\n",
    "        # For simplicity, we'll place it to the right of the person\n",
    "        ad_x = x + w + 20  # 20 pixels padding\n",
    "        ad_y = y\n",
    "        \n",
    "        return (ad_x, ad_y)\n",
    "    \n",
    "    def place_ad(self, frame, ad_image, position, scale=0.5):\n",
    "        \"\"\"Place the ad in the frame at the specified position\"\"\"\n",
    "        # Resize ad image\n",
    "        h, w = ad_image.shape[:2]\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        resized_ad = cv2.resize(ad_image, (new_w, new_h))\n",
    "        \n",
    "        # Create a copy of the frame\n",
    "        result = frame.copy()\n",
    "        \n",
    "        # Get ad position\n",
    "        x, y = position\n",
    "        \n",
    "        # Check if the ad fits within the frame\n",
    "        frame_h, frame_w = frame.shape[:2]\n",
    "        if x + new_w > frame_w:\n",
    "            x = frame_w - new_w - 10\n",
    "        if y + new_h > frame_h:\n",
    "            y = frame_h - new_h - 10\n",
    "        \n",
    "        # Create a region of interest\n",
    "        roi = result[y:y+new_h, x:x+new_w]\n",
    "        \n",
    "        # Create a mask for the ad\n",
    "        if resized_ad.shape[2] == 4:  # If ad has alpha channel\n",
    "            ad_mask = resized_ad[:, :, 3] / 255.0\n",
    "            ad_rgb = resized_ad[:, :, :3]\n",
    "            \n",
    "            # Blend the ad with the frame\n",
    "            for c in range(3):\n",
    "                roi[:, :, c] = roi[:, :, c] * (1 - ad_mask) + ad_rgb[:, :, c] * ad_mask\n",
    "        else:  # No alpha channel\n",
    "            result[y:y+new_h, x:x+new_w] = resized_ad\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_video(self, video_path, ad_image, output_path, max_frames=None):\n",
    "        \"\"\"Process the entire video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if max_frames is not None:\n",
    "            frame_count = min(frame_count, max_frames)\n",
    "        \n",
    "        # Create video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Initialize variables for RVM\n",
    "        prev_frame = None\n",
    "        prev_fgr = None\n",
    "        prev_pha = None\n",
    "        prev_state = None\n",
    "        \n",
    "        # Process each frame\n",
    "        frame_idx = 0\n",
    "        ad_position = None\n",
    "        \n",
    "        from tqdm.notebook import tqdm\n",
    "        for _ in tqdm(range(frame_count), desc=\"Processing video\"):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Every 30 frames, re-detect people and ad placement\n",
    "            if frame_idx % 30 == 0:\n",
    "                # Segment people\n",
    "                mask = self.segment_people(frame)\n",
    "                \n",
    "                # Find ad placement\n",
    "                ad_position = self.find_ad_placement(mask)\n",
    "            \n",
    "            # If we found a position for the ad, place it\n",
    "            if ad_position is not None:\n",
    "                frame = self.place_ad(frame, ad_image, ad_position)\n",
    "            \n",
    "            # Generate matte for the next frame\n",
    "            if frame_idx % 5 == 0:  # Only update matte every 5 frames to save computation\n",
    "                alpha_matte, foreground, curr_frame, curr_fgr, curr_pha, curr_state = self.generate_matte(\n",
    "                    frame, prev_frame, prev_fgr, prev_pha, prev_state\n",
    "                )\n",
    "                \n",
    "                # Update previous frame variables\n",
    "                prev_frame = curr_frame\n",
    "                prev_fgr = curr_fgr\n",
    "                prev_pha = curr_pha\n",
    "                prev_state = curr_state\n",
    "            \n",
    "            # Write the frame\n",
    "            out.write(frame)\n",
    "            frame_idx += 1\n",
    "        \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Video with CrowdFace\n",
    "\n",
    "Now let's process the uploaded video with our CrowdFace pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = CrowdFacePipeline(sam_model, sam_processor, rvm_model)\n",
    "\n",
    "# Set output path\n",
    "output_path = \"output_video.mp4\"\n",
    "\n",
    "# Process a subset of frames for demonstration (adjust max_frames as needed)\n",
    "processed_video = pipeline.process_video(video_path, ad_image, output_path, max_frames=300)\n",
    "\n",
    "print(f\"Video processing complete! Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Processed Video\n",
    "\n",
    "Download the processed video with the advertisement integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "else:\n",
    "    print(f\"Video saved locally at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Video Preview\n",
    "\n",
    "Let's display a preview of the processed video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def show_video(video_path):\n",
    "    video_file = open(video_path, \"rb\")\n",
    "    video_bytes = video_file.read()\n",
    "    video_b64 = b64encode(video_bytes).decode()\n",
    "    video_tag = f'<video width=\"640\" height=\"360\" controls><source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\"></video>'\n",
    "    return HTML(video_tag)\n",
    "\n",
    "show_video(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use CrowdFace for neural-adaptive crowd segmentation with contextual pixel-space advertisement integration. The system uses state-of-the-art models:\n",
    "\n",
    "1. **SAM2 (Segment Anything Model 2)** for precise crowd detection and segmentation\n",
    "2. **RVM (Robust Video Matting)** for high-quality alpha matte generation\n",
    "3. **BAGEL** (ByteDance Ad Generation and Embedding Library) concepts for intelligent ad placement\n",
    "\n",
    "The pipeline leverages GPU acceleration for real-time processing and can be extended with additional features such as:\n",
    "\n",
    "- Multiple advertisement placement\n",
    "- Dynamic ad content based on scene context\n",
    "- Improved tracking for consistent ad placement\n",
    "- Custom ad styles and animations\n",
    "\n",
    "For more information, visit the [CrowdFace GitHub repository](https://github.com/BlackBoyZeus/CrowdFace)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}