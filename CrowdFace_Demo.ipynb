{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CrowdFace Demo: Neural-Adaptive Crowd Segmentation with Contextual Pixel-Space Advertisement Integration\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BlackBoyZeus/CrowdFace/blob/main/CrowdFace_Demo.ipynb)\n",
        "\n",
        "This notebook demonstrates the CrowdFace system, which combines state-of-the-art segmentation models with contextual advertisement placement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision opencv-python transformers diffusers accelerate safetensors huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the Bagel repository\n",
        "!git clone https://github.com/ByteDance-Seed/Bagel.git\n",
        "# Add Bagel to the Python path\n",
        "import sys\n",
        "sys.path.append('Bagel')\n",
        "\n",
        "# Clone the CrowdFace repository to get the implementation files\n",
        "!git clone https://github.com/BlackBoyZeus/CrowdFace.git\n",
        "sys.path.append('CrowdFace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from huggingface_hub import snapshot_download\n",
        "from copy import deepcopy\n",
        "from typing import Dict, List, Optional, Tuple, Union, Any\n",
        "\n",
        "# For Google Colab, we'll use the secrets module to securely handle API keys\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "    print(\"Running in Google Colab with secure API key handling\")\n",
        "except:\n",
        "    HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')\n",
        "    print(\"Running in local environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading\n",
        "\n",
        "### 1. Load SAM2 (Segment Anything Model 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import SamModel, SamProcessor\n",
        "\n",
        "# Load SAM2 model and processor\n",
        "model_id = \"facebook/sam2\"\n",
        "sam_processor = SamProcessor.from_pretrained(model_id, token=HUGGINGFACE_TOKEN)\n",
        "sam_model = SamModel.from_pretrained(model_id, token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sam_model = sam_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load RVM (Robust Video Matting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone RVM repository if not already present\n",
        "!git clone https://github.com/PeterL1n/RobustVideoMatting.git\n",
        "sys.path.append('RobustVideoMatting')\n",
        "\n",
        "try:\n",
        "    from model import MattingNetwork\n",
        "    \n",
        "    # Load RVM model\n",
        "    rvm_model = MattingNetwork('mobilenetv3').eval().to(device)\n",
        "    \n",
        "    # Download RVM weights\n",
        "    if not os.path.exists('rvm_mobilenetv3.pth'):\n",
        "        !wget https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth\n",
        "    \n",
        "    # Load weights\n",
        "    rvm_model.load_state_dict(torch.load('rvm_mobilenetv3.pth', map_location=device))\n",
        "    print(\"RVM model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading RVM model: {e}\")\n",
        "    print(\"Will use fallback methods for matting\")\n",
        "    rvm_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Load BAGEL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and set up BAGEL model\n",
        "save_dir = \"models/BAGEL-7B-MoT\"\n",
        "repo_id = \"ByteDance-Seed/BAGEL-7B-MoT\"\n",
        "cache_dir = save_dir + \"/cache\"\n",
        "\n",
        "try:\n",
        "    print(\"Downloading BAGEL model (this may take some time)...\")\n",
        "    snapshot_download(cache_dir=cache_dir,\n",
        "      local_dir=save_dir,\n",
        "      repo_id=repo_id,\n",
        "      local_dir_use_symlinks=False,\n",
        "      resume_download=True,\n",
        "      token=HUGGINGFACE_TOKEN,\n",
        "      allow_patterns=[\"*.json\", \"*.safetensors\", \"*.bin\", \"*.py\", \"*.md\", \"*.txt\"],\n",
        "    )\n",
        "    print(\"BAGEL model downloaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading BAGEL model: {e}\")\n",
        "    print(\"Will use fallback methods for scene understanding and ad placement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BAGEL model\n",
        "try:\n",
        "    from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
        "    from Bagel.data.transforms import ImageTransform\n",
        "    from Bagel.data.data_utils import add_special_tokens\n",
        "    from Bagel.modeling.bagel import (\n",
        "        BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n",
        "    )\n",
        "    from Bagel.modeling.qwen2 import Qwen2Tokenizer\n",
        "    from Bagel.modeling.bagel.qwen2_navit import NaiveCache\n",
        "    from Bagel.modeling.autoencoder import load_ae\n",
        "    from Bagel.inferencer import InterleaveInferencer\n",
        "    \n",
        "    model_path = save_dir\n",
        "    \n",
        "    # LLM config preparing\n",
        "    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
        "    llm_config.qk_norm = True\n",
        "    llm_config.tie_word_embeddings = False\n",
        "    llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
        "    \n",
        "    # ViT config preparing\n",
        "    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
        "    vit_config.rope = False\n",
        "    vit_config.num_hidden_layers = vit_config.num_hidden_layers - 1\n",
        "    \n",
        "    # VAE loading\n",
        "    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
        "    \n",
        "    # Bagel config preparing\n",
        "    config = BagelConfig(\n",
        "        visual_gen=True,\n",
        "        visual_und=True,\n",
        "        llm_config=llm_config, \n",
        "        vit_config=vit_config,\n",
        "        vae_config=vae_config,\n",
        "        vit_max_num_patch_per_side=70,\n",
        "        connector_act='gelu_pytorch_tanh',\n",
        "        latent_patch_size=2,\n",
        "        max_latent_size=64,\n",
        "    )\n",
        "    \n",
        "    # Initialize model with empty weights\n",
        "    with init_empty_weights():\n",
        "        language_model = Qwen2ForCausalLM(llm_config)\n",
        "        vit_model = SiglipVisionModel(vit_config)\n",
        "        model = Bagel(language_model, vit_model, config)\n",
        "        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
        "    \n",
        "    # Load tokenizer and add special tokens\n",
        "    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
        "    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
        "    \n",
        "    # Set up transforms\n",
        "    vae_transform = ImageTransform(1024, 512, 16)\n",
        "    vit_transform = ImageTransform(980, 224, 14)\n",
        "    \n",
        "    # Set up device map for model loading\n",
        "    device_map = infer_auto_device_map(\n",
        "        model,\n",
        "        max_memory={i: \"80GiB\" for i in range(torch.cuda.device_count())},\n",
        "        no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
        "    )\n",
        "    \n",
        "    # Define modules that should be on the same device\n",
        "    same_device_modules = [\n",
        "        'language_model.model.embed_tokens',\n",
        "        'time_embedder',\n",
        "        'latent_pos_embed',\n",
        "        'vae2llm',\n",
        "        'llm2vae',\n",
        "        'connector',\n",
        "    ]\n",
        "    \n",
        "    # Load model weights\n",
        "    model = load_checkpoint_and_dispatch(\n",
        "        model, \n",
        "        os.path.join(model_path, \"pytorch_model.bin\"),\n",
        "        device_map=device_map,\n",
        "        offload_folder=None,\n",
        "        offload_state_dict=False,\n",
        "        same_device_modules=same_device_modules,\n",
        "    )\n",
        "    \n",
        "    # Initialize the inferencer\n",
        "    bagel_inferencer = InterleaveInferencer(\n",
        "        model=model, \n",
        "        vae_model=vae_model, \n",
        "        tokenizer=tokenizer, \n",
        "        vae_transform=vae_transform, \n",
        "        vit_transform=vit_transform, \n",
        "        new_token_ids=new_token_ids\n",
        "    )\n",
        "    \n",
        "    print(\"BAGEL model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing BAGEL model: {e}\")\n",
        "    print(\"Will use fallback methods for scene understanding and ad placement.\")\n",
        "    bagel_inferencer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import CrowdFace Components\n",
        "\n",
        "Now let's import the CrowdFace components from the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import CrowdFace components\n",
        "from CrowdFace.src.python.bagel.scene_understanding import BAGELSceneUnderstanding\n",
        "from CrowdFace.src.python.bagel.ad_placement import BAGELAdPlacement\n",
        "from CrowdFace.src.python.bagel.ad_optimization import BAGELAdOptimization\n",
        "from CrowdFace.src.python.crowdface_pipeline import CrowdFacePipeline\n",
        "\n",
        "# Initialize the CrowdFace pipeline\n",
        "pipeline = CrowdFacePipeline(\n",
        "    sam_model=sam_model,\n",
        "    sam_processor=sam_processor,\n",
        "    rvm_model=rvm_model if 'rvm_model' in locals() else None,\n",
        "    bagel_inferencer=bagel_inferencer if 'bagel_inferencer' in locals() else None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload and Process a Video\n",
        "\n",
        "Now let's upload a video and process it with our CrowdFace pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab, add file upload widget\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    video_path = next(iter(uploaded.keys()))\n",
        "    print(f\"Uploaded video: {video_path}\")\n",
        "except ImportError:\n",
        "    # If not in Colab, use a sample video\n",
        "    # Download a sample video\n",
        "    !wget -O sample_video.mp4 https://pixabay.com/videos/download/video-41758_source.mp4?attachment\n",
        "    video_path = 'sample_video.mp4'\n",
        "    print(f\"Using sample video: {video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or download a sample ad image\n",
        "try:\n",
        "    # Try to upload an ad image\n",
        "    from google.colab import files\n",
        "    print(\"Upload an ad image:\")\n",
        "    uploaded = files.upload()\n",
        "    ad_path = next(iter(uploaded.keys()))\n",
        "    print(f\"Uploaded ad image: {ad_path}\")\n",
        "except (ImportError, StopIteration):\n",
        "    # If not in Colab or no file uploaded, create a sample ad\n",
        "    ad_img = np.ones((300, 500, 4), dtype=np.uint8) * 255\n",
        "    # Add some text\n",
        "    cv2.putText(ad_img, \"SAMPLE AD\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255, 255), 5)\n",
        "    cv2.imwrite('sample_ad.png', ad_img)\n",
        "    ad_path = 'sample_ad.png'\n",
        "    print(\"Created a sample ad image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the video\n",
        "output_path = 'output_video.mp4'\n",
        "pipeline.process_video(\n",
        "    video_path=video_path,\n",
        "    ad_image=ad_path,\n",
        "    output_path=output_path,\n",
        "    max_frames=100  # Limit to 100 frames for faster processing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display the Results\n",
        "\n",
        "Let's display the output video to see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "# Display the output video\n",
        "Video(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab, add download option\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_path)\n",
        "except ImportError:\n",
        "    print(f\"Output video saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated the CrowdFace system, which combines:\n",
        "\n",
        "1. **SAM2** for precise crowd segmentation\n",
        "2. **RVM** for high-quality video matting\n",
        "3. **BAGEL** for intelligent scene understanding and ad placement\n",
        "\n",
        "This integration creates a sophisticated product that's difficult to replicate because it combines multiple cutting-edge AI models in a way that enhances each component. The result is a system that not only places ads in videos but does so with an understanding of scene context, optimal placement, and content optimization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
