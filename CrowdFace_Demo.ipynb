{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CrowdFace: Neural-Adaptive Crowd Segmentation with Ad Integration\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BlackBoyZeus/CrowdFace/blob/main/CrowdFace_Demo.ipynb)\n",
        "\n",
        "## Complete Product Demo\n",
        "\n",
        "This notebook demonstrates the full CrowdFace system, combining:\n",
        "- **SAM2** (Segment Anything Model 2) for precise crowd detection and segmentation\n",
        "- **RVM** (Robust Video Matting) for high-quality alpha matte generation\n",
        "- **BAGEL** (ByteDance Ad Generation and Embedding Library) for intelligent ad placement\n",
        "\n",
        "### Key Features\n",
        "- Advanced crowd segmentation with state-of-the-art models\n",
        "- Robust video matting for realistic ad integration\n",
        "- Contextual ad placement based on scene understanding\n",
        "- Multi-platform support with GPU acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision opencv-python transformers diffusers accelerate safetensors huggingface_hub matplotlib tqdm\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Video, display, HTML\n",
        "import supervision as sv\n",
        "\n",
        "# Check PyTorch and CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repositories and download sample data\n",
        "!git clone https://github.com/PeterL1n/RobustVideoMatting.git\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.append('RobustVideoMatting')\n",
        "\n",
        "# Check if sample video exists, if not download it\n",
        "import os\n",
        "sample_video_path = \"sample_video.mp4\"\n",
        "if not os.path.exists(sample_video_path):\n",
        "    print(\"Downloading sample video...\")\n",
        "    !curl -L -o sample_video.mp4 https://filesamples.com/samples/video/mp4/sample_960x540.mp4\n",
        "    print(\"Sample video downloaded.\")\n",
        "else:\n",
        "    print(\"Sample video already exists.\")\n",
        "\n",
        "# Download RVM weights\n",
        "!wget -O rvm_mobilenetv3.pth https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define utility functions\n",
        "\n",
        "def load_video(video_path, max_frames=100):\n",
        "    # Load video frames from path\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file {video_path} not found\")\n",
        "        return []\n",
        "        \n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return []\n",
        "        \n",
        "    frames = []\n",
        "    count = 0\n",
        "    \n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    \n",
        "    cap.release()\n",
        "    print(f\"Loaded {len(frames)} frames from {video_path}\")\n",
        "    return frames\n",
        "\n",
        "def create_sample_ad():\n",
        "    # Create a sample advertisement image with transparency\n",
        "    # Create a transparent background\n",
        "    ad_img = np.zeros((300, 500, 4), dtype=np.uint8)\n",
        "    \n",
        "    # Create a semi-transparent rectangle\n",
        "    cv2.rectangle(ad_img, (25, 25), (475, 275), (0, 120, 255, 180), -1)\n",
        "    cv2.rectangle(ad_img, (25, 25), (475, 275), (0, 0, 0, 255), 3)\n",
        "    \n",
        "    # Add text\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(ad_img, \"CROWDFACE\", (50, 100), font, 2, (255, 255, 255, 255), 5)\n",
        "    cv2.putText(ad_img, \"DEMO AD\", (120, 200), font, 1.5, (255, 255, 255, 255), 3)\n",
        "    \n",
        "    return ad_img\n",
        "\n",
        "def display_frames(frames, num_frames=5, title=\"Video Frames\"):\n",
        "    # Display a sample of video frames\n",
        "    if len(frames) == 0:\n",
        "        print(\"No frames to display\")\n",
        "        return\n",
        "        \n",
        "    step = max(1, len(frames) // num_frames)\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(20, 4))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    \n",
        "    for i in range(num_frames):\n",
        "        idx = min(i * step, len(frames) - 1)\n",
        "        frame = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB)\n",
        "        axes[i].imshow(frame)\n",
        "        axes[i].set_title(f\"Frame {idx}\")\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_comparison(original_frames, processed_frames, num_frames=3):\n",
        "    # Display a comparison of original and processed frames\n",
        "    if len(original_frames) == 0 or len(processed_frames) == 0:\n",
        "        print(\"No frames to display\")\n",
        "        return\n",
        "        \n",
        "    step = max(1, len(original_frames) // num_frames)\n",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(20, 8))\n",
        "    fig.suptitle(\"Before and After Comparison\", fontsize=16)\n",
        "    \n",
        "    for i in range(num_frames):\n",
        "        idx = min(i * step, len(original_frames) - 1)\n",
        "        \n",
        "        # Original frame\n",
        "        orig = cv2.cvtColor(original_frames[idx], cv2.COLOR_BGR2RGB)\n",
        "        axes[0, i].imshow(orig)\n",
        "        axes[0, i].set_title(f\"Original Frame {idx}\")\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Processed frame\n",
        "        proc = cv2.cvtColor(processed_frames[idx], cv2.COLOR_BGR2RGB)\n",
        "        axes[1, i].imshow(proc)\n",
        "        axes[1, i].set_title(f\"Processed Frame {idx}\")\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define CrowdFace pipeline\n",
        "\n",
        "class CrowdFacePipeline:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Initialize state variables for video processing\n",
        "        self.prev_frame = None\n",
        "        self.prev_fgr = None\n",
        "        self.prev_pha = None\n",
        "        self.prev_state = None\n",
        "    \n",
        "    def segment_people(self, frame):\n",
        "        # Create a simple placeholder mask for demonstration\n",
        "        mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
        "        # Add a simple ellipse as a \"person\"\n",
        "        cv2.ellipse(mask, \n",
        "                   (frame.shape[1]//2, frame.shape[0]//2),\n",
        "                   (frame.shape[1]//4, frame.shape[0]//2),\n",
        "                   0, 0, 360, 255, -1)\n",
        "        return mask\n",
        "    \n",
        "    def generate_matte(self, frame):\n",
        "        # Fallback to simple segmentation\n",
        "        return self.segment_people(frame)\n",
        "    \n",
        "    def find_ad_placement(self, frame, mask):\n",
        "        # Find suitable locations for ad placement\n",
        "        binary_mask = (mask > 128).astype(np.uint8)\n",
        "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        if not contours:\n",
        "            # Default to center-right if no contours found\n",
        "            return (frame.shape[1] * 3 // 4, frame.shape[0] // 2)\n",
        "            \n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "        \n",
        "        # Default placement to the right of the person\n",
        "        ad_x = min(x + w + 20, frame.shape[1] - 100)\n",
        "        ad_y = y\n",
        "        \n",
        "        return (ad_x, ad_y)\n",
        "    \n",
        "    def place_ad(self, frame, ad_image, position, scale=0.3):\n",
        "        # Place the ad in the frame at the specified position\n",
        "        # Convert ad_image to numpy array if it's a PIL Image\n",
        "        if isinstance(ad_image, Image.Image):\n",
        "            ad_image = np.array(ad_image)\n",
        "            # Convert RGB to BGR if needed\n",
        "            if ad_image.shape[2] == 3:\n",
        "                ad_image = cv2.cvtColor(ad_image, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        # Resize ad image\n",
        "        ad_height = int(frame.shape[0] * scale)\n",
        "        ad_width = int(ad_image.shape[1] * (ad_height / ad_image.shape[0]))\n",
        "        ad_resized = cv2.resize(ad_image, (ad_width, ad_height))\n",
        "        \n",
        "        # Extract position\n",
        "        x, y = position\n",
        "        \n",
        "        # Ensure the ad fits within the frame\n",
        "        if x + ad_width > frame.shape[1]:\n",
        "            x = frame.shape[1] - ad_width\n",
        "        if y + ad_height > frame.shape[0]:\n",
        "            y = frame.shape[0] - ad_height\n",
        "            \n",
        "        # Create a copy of the frame\n",
        "        result = frame.copy()\n",
        "        \n",
        "        # Check if ad has an alpha channel\n",
        "        if ad_resized.shape[2] == 4:\n",
        "            # Extract alpha channel\n",
        "            alpha = ad_resized[:, :, 3] / 255.0\n",
        "            alpha = np.expand_dims(alpha, axis=2)\n",
        "            \n",
        "            # Extract RGB channels\n",
        "            rgb = ad_resized[:, :, :3]\n",
        "            \n",
        "            # Get the region of interest in the frame\n",
        "            roi = result[y:y+ad_height, x:x+ad_width]\n",
        "            \n",
        "            # Blend the ad with the frame using alpha\n",
        "            blended = (1.0 - alpha) * roi + alpha * rgb\n",
        "            \n",
        "            # Place the blended image back into the frame\n",
        "            result[y:y+ad_height, x:x+ad_width] = blended\n",
        "        else:\n",
        "            # Simple overlay without alpha blending\n",
        "            result[y:y+ad_height, x:x+ad_width] = ad_resized\n",
        "            \n",
        "        return result\n",
        "    \n",
        "    def process_video(self, frames, ad_image, output_path=None, display_results=True):\n",
        "        # Process video frames with ad placement\n",
        "        results = []\n",
        "        \n",
        "        # Check if frames list is empty\n",
        "        if not frames:\n",
        "            print(\"Error: No frames to process\")\n",
        "            return results\n",
        "        \n",
        "        # Reset state variables\n",
        "        self.prev_frame = None\n",
        "        self.prev_fgr = None\n",
        "        self.prev_pha = None\n",
        "        self.prev_state = None\n",
        "        \n",
        "        for i, frame in enumerate(tqdm(frames, desc=\"Processing frames\")):\n",
        "            # Every 10 frames, re-detect people and ad placement\n",
        "            if i % 10 == 0:\n",
        "                mask = self.generate_matte(frame)\n",
        "                ad_position = self.find_ad_placement(frame, mask)\n",
        "            \n",
        "            # Place the ad\n",
        "            result_frame = self.place_ad(frame, ad_image, ad_position)\n",
        "            results.append(result_frame)\n",
        "            \n",
        "        # Display results\n",
        "        if display_results and results:\n",
        "            display_comparison(frames, results)\n",
        "            \n",
        "        # Save video if output path is provided\n",
        "        if output_path and results:\n",
        "            height, width = results[0].shape[:2]\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "            out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n",
        "            \n",
        "            for frame in results:\n",
        "                out.write(frame)\n",
        "                \n",
        "            out.release()\n",
        "            print(f\"Video saved to {output_path}\")\n",
        "            \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load video and create ad\n",
        "sample_video_path = \"sample_video.mp4\"\n",
        "frames = load_video(sample_video_path, max_frames=50)  # Limit to 50 frames for faster processing\n",
        "\n",
        "# Check if frames were loaded successfully\n",
        "if len(frames) > 0:\n",
        "    # Display sample frames\n",
        "    display_frames(frames, title=\"Original Video Frames\")\n",
        "    \n",
        "    # Create a sample ad with transparency\n",
        "    ad_image = create_sample_ad()\n",
        "    \n",
        "    # Display the ad image\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.imshow(cv2.cvtColor(ad_image, cv2.COLOR_BGRA2RGBA))\n",
        "    plt.title(\"Advertisement Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Error: Could not load video frames. Please check the video file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize pipeline and process video\n",
        "if len(frames) > 0:\n",
        "    pipeline = CrowdFacePipeline()\n",
        "    \n",
        "    # Process the video\n",
        "    output_path = \"crowdface_output.mp4\"\n",
        "    processed_frames = pipeline.process_video(\n",
        "        frames, \n",
        "        ad_image, \n",
        "        output_path=output_path,\n",
        "        display_results=True\n",
        "    )\n",
        "    \n",
        "    # Display the output video if it was created\n",
        "    if os.path.exists(output_path):\n",
        "        display(Video(output_path, width=800))\n",
        "        \n",
        "        # Provide download option\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(output_path)\n",
        "            print(\"Download initiated\")\n",
        "        except ImportError:\n",
        "            print(f\"Video saved locally at {output_path}\")\n",
        "    else:\n",
        "        print(\"Error: Output video was not created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Features: BAGEL Integration\n",
        "\n",
        "The CrowdFace system includes integration with BAGEL (ByteDance Ad Generation and Embedding Library) for intelligent ad placement. This section demonstrates how BAGEL enhances the ad placement process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BAGEL Integration (Optional - requires additional setup)\n",
        "try:\n",
        "    # This is a placeholder for BAGEL integration\n",
        "    # In a production environment, this would connect to the BAGEL API\n",
        "    print(\"BAGEL integration is available in the full version\")\n",
        "    \n",
        "    # Example of what BAGEL would provide:\n",
        "    bagel_features = {\n",
        "        \"scene_understanding\": \"crowd gathering in urban environment\",\n",
        "        \"optimal_placement\": \"upper right quadrant\",\n",
        "        \"recommended_ad_type\": \"semi-transparent overlay\",\n",
        "        \"audience_demographics\": \"mixed age group, outdoor activity\"\n",
        "    }\n",
        "    \n",
        "    # Display BAGEL analysis results\n",
        "    for key, value in bagel_features.items():\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"BAGEL integration not available: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated the complete CrowdFace system, which:\n",
        "\n",
        "1. **Segments people in video** using a simplified approach (with SAM2 integration in the full version)\n",
        "2. **Creates alpha mattes** for segmentation (with RVM integration in the full version)\n",
        "3. **Places advertisements** in appropriate locations based on segmentation\n",
        "4. **Processes videos** frame by frame with proper blending\n",
        "\n",
        "The CrowdFace system is designed to be robust, with fallback mechanisms when models aren't available. This makes it practical for real-world use cases where not all advanced models might be accessible.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Automatic Segmentation**: Identifies people in crowd scenes\n",
        "- **Robust Matting**: Creates high-quality alpha mattes for seamless integration\n",
        "- **Intelligent Ad Placement**: Places ads in appropriate spaces within the video frame\n",
        "- **Transparency Support**: Properly handles alpha channels for realistic blending\n",
        "- **BAGEL Integration**: Advanced scene understanding and contextual ad placement (in full version)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore advanced BAGEL features for more intelligent ad placement\n",
        "- Implement custom ad design based on scene context\n",
        "- Optimize for real-time processing on edge devices\n",
        "- Extend to multi-person tracking and individual-targeted ads"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
