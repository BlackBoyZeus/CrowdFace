{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CrowdFace: Neural-Adaptive Crowd Segmentation with Contextual Pixel-Space Advertisement Integration",
        "",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BlackBoyZeus/CrowdFace/blob/main/CrowdFace_Demo.ipynb)",
        "",
        "This notebook demonstrates the complete CrowdFace system, which combines SAM2 for segmentation, RVM for matting, and BAGEL for intelligent ad placement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-run setup - This cell will automatically download and set up everything needed",
        "# No user interaction required!",
        "",
        "# Install all required packages",
        "!pip install -q torch torchvision opencv-python transformers diffusers accelerate safetensors huggingface_hub matplotlib tqdm",
        "",
        "# Clone necessary repositories",
        "!git clone -q https://github.com/BlackBoyZeus/CrowdFace.git",
        "!git clone -q https://github.com/PeterL1n/RobustVideoMatting.git",
        "",
        "# Download a sample video for testing",
        "!wget -q -O sample_video.mp4 https://pixabay.com/videos/download/video-41758_source.mp4?attachment",
        "",
        "# Download RVM weights",
        "!wget -q -O rvm_mobilenetv3.pth https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth",
        "",
        "print(\"\u2705 Setup complete! All dependencies and models downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries",
        "import os",
        "import sys",
        "import torch",
        "import numpy as np",
        "import cv2",
        "from PIL import Image",
        "import matplotlib.pyplot as plt",
        "from tqdm.notebook import tqdm",
        "from IPython.display import Video, display, HTML",
        "",
        "# Add repositories to path",
        "sys.path.append('CrowdFace')",
        "sys.path.append('RobustVideoMatting')",
        "",
        "# Display a success message with styling",
        "display(HTML(\"<div style='background:#d4edda; color:#155724; padding:10px; border-radius:5px; margin-bottom:10px'><b>\u2705 Libraries imported successfully!</b></div>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define utility functions for the demo",
        "",
        "def load_video(video_path, max_frames=100):",
        "    # Load video frames from path",
        "    cap = cv2.VideoCapture(video_path)",
        "    frames = []",
        "    count = 0",
        "    ",
        "    while cap.isOpened() and count < max_frames:",
        "        ret, frame = cap.read()",
        "        if not ret:",
        "            break",
        "        frames.append(frame)",
        "        count += 1",
        "    ",
        "    cap.release()",
        "    print(f\"\u2705 Loaded {len(frames)} frames from {video_path}\")",
        "    return frames",
        "",
        "def create_sample_ad():",
        "    # Create a sample advertisement image with transparency",
        "    # Create a transparent background",
        "    ad_img = np.zeros((300, 500, 4), dtype=np.uint8)",
        "    ",
        "    # Create a semi-transparent rectangle",
        "    cv2.rectangle(ad_img, (25, 25), (475, 275), (0, 120, 255, 180), -1)",
        "    cv2.rectangle(ad_img, (25, 25), (475, 275), (0, 0, 0, 255), 3)",
        "    ",
        "    # Add text",
        "    font = cv2.FONT_HERSHEY_SIMPLEX",
        "    cv2.putText(ad_img, \"CROWDFACE\", (50, 100), font, 2, (255, 255, 255, 255), 5)",
        "    cv2.putText(ad_img, \"DEMO AD\", (120, 200), font, 1.5, (255, 255, 255, 255), 3)",
        "    ",
        "    return ad_img",
        "",
        "def display_frames(frames, num_frames=5, title=\"Video Frames\"):",
        "    # Display a sample of video frames",
        "    if len(frames) == 0:",
        "        print(\"No frames to display\")",
        "        return",
        "        ",
        "    step = max(1, len(frames) // num_frames)",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(20, 4))",
        "    fig.suptitle(title, fontsize=16)",
        "    ",
        "    for i in range(num_frames):",
        "        idx = min(i * step, len(frames) - 1)",
        "        frame = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB)",
        "        axes[i].imshow(frame)",
        "        axes[i].set_title(f\"Frame {idx}\")",
        "        axes[i].axis('off')",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "def display_comparison(original_frames, processed_frames, num_frames=3):",
        "    # Display a comparison of original and processed frames",
        "    if len(original_frames) == 0 or len(processed_frames) == 0:",
        "        print(\"No frames to display\")",
        "        return",
        "        ",
        "    step = max(1, len(original_frames) // num_frames)",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(20, 8))",
        "    fig.suptitle(\"Before and After Comparison\", fontsize=16)",
        "    ",
        "    for i in range(num_frames):",
        "        idx = min(i * step, len(original_frames) - 1)",
        "        ",
        "        # Original frame",
        "        orig = cv2.cvtColor(original_frames[idx], cv2.COLOR_BGR2RGB)",
        "        axes[0, i].imshow(orig)",
        "        axes[0, i].set_title(f\"Original Frame {idx}\")",
        "        axes[0, i].axis('off')",
        "        ",
        "        # Processed frame",
        "        proc = cv2.cvtColor(processed_frames[idx], cv2.COLOR_BGR2RGB)",
        "        axes[1, i].imshow(proc)",
        "        axes[1, i].set_title(f\"Processed Frame {idx}\")",
        "        axes[1, i].axis('off')",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the CrowdFace pipeline for video processing",
        "",
        "class CrowdFacePipeline:",
        "    def __init__(self):",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
        "        print(f\"Using device: {self.device}\")",
        "        ",
        "        # Initialize models",
        "        self.sam_model, self.sam_processor = self._load_sam_model()",
        "        self.rvm_model = self._load_rvm_model()",
        "        ",
        "        # Initialize state variables for video processing",
        "        self.prev_frame = None",
        "        self.prev_fgr = None",
        "        self.prev_pha = None",
        "        self.prev_state = None",
        "    ",
        "    def _load_sam_model(self):",
        "        # Load SAM2 model for segmentation",
        "        try:",
        "            from transformers import SamModel, SamProcessor",
        "            ",
        "            print(\"Loading SAM2 model...\")",
        "            model_id = \"facebook/sam2\"",
        "            ",
        "            # Try to load the model without a token first",
        "            try:",
        "                processor = SamProcessor.from_pretrained(model_id)",
        "                model = SamModel.from_pretrained(model_id)",
        "            except Exception:",
        "                # If that fails, try with a dummy token (will work for cached models)",
        "                processor = SamProcessor.from_pretrained(model_id, token=\"dummy\")",
        "                model = SamModel.from_pretrained(model_id, token=\"dummy\")",
        "            ",
        "            model = model.to(self.device)",
        "            print(\"\u2705 SAM2 model loaded successfully\")",
        "            return model, processor",
        "        except Exception as e:",
        "            print(f\"\u26a0\ufe0f Error loading SAM2 model: {e}\")",
        "            print(\"Using placeholder segmentation\")",
        "            return None, None",
        "    ",
        "    def _load_rvm_model(self):",
        "        # Load RVM model for video matting",
        "        try:",
        "            sys.path.append('RobustVideoMatting')",
        "            from model import MattingNetwork",
        "            ",
        "            # Load RVM model",
        "            model = MattingNetwork('mobilenetv3').eval().to(self.device)",
        "            ",
        "            # Load weights",
        "            model.load_state_dict(torch.load('rvm_mobilenetv3.pth', map_location=self.device))",
        "            print(\"\u2705 RVM model loaded successfully\")",
        "            return model",
        "        except Exception as e:",
        "            print(f\"\u26a0\ufe0f Error loading RVM model: {e}\")",
        "            print(\"Using placeholder matting\")",
        "            return None",
        "    ",
        "    def segment_people(self, frame):",
        "        # Segment people in the frame using SAM2",
        "        if self.sam_model is None or self.sam_processor is None:",
        "            # Create a simple placeholder mask for demonstration",
        "            mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)",
        "            # Add a simple ellipse as a \"person\"",
        "            cv2.ellipse(mask, ",
        "                       (frame.shape[1]//2, frame.shape[0]//2),",
        "                       (frame.shape[1]//4, frame.shape[0]//2),",
        "                       0, 0, 360, 255, -1)",
        "            return mask",
        "            ",
        "        # Convert frame to RGB if it's in BGR format",
        "        if isinstance(frame, np.ndarray) and frame.shape[2] == 3:",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)",
        "        else:",
        "            rgb_frame = frame",
        "            ",
        "        # Process the image with SAM",
        "        inputs = self.sam_processor(rgb_frame, return_tensors=\"pt\").to(self.device)",
        "        ",
        "        # Generate automatic mask predictions",
        "        with torch.no_grad():",
        "            outputs = self.sam_model(**inputs)",
        "            ",
        "        # Get the predicted masks",
        "        masks = self.sam_processor.image_processor.post_process_masks(",
        "            outputs.pred_masks.cpu(),",
        "            inputs[\"original_sizes\"].cpu(),",
        "            inputs[\"reshaped_input_sizes\"].cpu()",
        "        )",
        "        ",
        "        # Take the largest mask as a person (simplified approach)",
        "        combined_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)",
        "        ",
        "        if len(masks) > 0 and len(masks[0]) > 0:",
        "            largest_mask = None",
        "            largest_area = 0",
        "            ",
        "            for mask in masks[0]:",
        "                mask_np = mask.numpy()",
        "                area = np.sum(mask_np)",
        "                if area > largest_area:",
        "                    largest_area = area",
        "                    largest_mask = mask_np",
        "                    ",
        "            if largest_mask is not None:",
        "                combined_mask = largest_mask.astype(np.uint8) * 255",
        "        ",
        "        return combined_mask",
        "    ",
        "    def generate_matte(self, frame):",
        "        # Generate alpha matte using RVM",
        "        if self.rvm_model is None:",
        "            # Fallback to simple segmentation",
        "            return self.segment_people(frame)",
        "            ",
        "        try:",
        "            # Convert frame to tensor",
        "            frame_tensor = torch.from_numpy(frame).float().permute(2, 0, 1).unsqueeze(0) / 255.0",
        "            frame_tensor = frame_tensor.to(self.device)",
        "            ",
        "            # Initialize previous frame and state if not provided",
        "            if self.prev_frame is None:",
        "                self.prev_frame = torch.zeros_like(frame_tensor)",
        "            if self.prev_fgr is None:",
        "                self.prev_fgr = torch.zeros_like(frame_tensor)",
        "            if self.prev_pha is None:",
        "                self.prev_pha = torch.zeros((1, 1, frame.shape[0], frame.shape[1]), device=self.device)",
        "                ",
        "            # Generate matte",
        "            with torch.no_grad():",
        "                fgr, pha, state = self.rvm_model(frame_tensor, self.prev_frame, self.prev_fgr, self.prev_pha, self.prev_state)",
        "                ",
        "            # Update state for next frame",
        "            self.prev_frame = frame_tensor",
        "            self.prev_fgr = fgr",
        "            self.prev_pha = pha",
        "            self.prev_state = state",
        "                ",
        "            # Convert alpha matte to numpy array",
        "            alpha_matte = pha[0, 0].cpu().numpy() * 255",
        "            alpha_matte = alpha_matte.astype(np.uint8)",
        "            ",
        "            return alpha_matte",
        "            ",
        "        except Exception as e:",
        "            print(f\"Error in RVM matting: {e}\")",
        "            # Fallback to segmentation mask",
        "            return self.segment_people(frame)",
        "    ",
        "    def find_ad_placement(self, frame, mask):",
        "        # Find suitable locations for ad placement",
        "        binary_mask = (mask > 128).astype(np.uint8)",
        "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)",
        "        ",
        "        if not contours:",
        "            # Default to center-right if no contours found",
        "            return (frame.shape[1] * 3 // 4, frame.shape[0] // 2)",
        "            ",
        "        largest_contour = max(contours, key=cv2.contourArea)",
        "        x, y, w, h = cv2.boundingRect(largest_contour)",
        "        ",
        "        # Default placement to the right of the person",
        "        ad_x = min(x + w + 20, frame.shape[1] - 100)",
        "        ad_y = y",
        "        ",
        "        return (ad_x, ad_y)",
        "    ",
        "    def place_ad(self, frame, ad_image, position, scale=0.3):",
        "        # Place the ad in the frame at the specified position",
        "        # Convert ad_image to numpy array if it's a PIL Image",
        "        if isinstance(ad_image, Image.Image):",
        "            ad_image = np.array(ad_image)",
        "            # Convert RGB to BGR if needed",
        "            if ad_image.shape[2] == 3:",
        "                ad_image = cv2.cvtColor(ad_image, cv2.COLOR_RGB2BGR)",
        "        ",
        "        # Resize ad image",
        "        ad_height = int(frame.shape[0] * scale)",
        "        ad_width = int(ad_image.shape[1] * (ad_height / ad_image.shape[0]))",
        "        ad_resized = cv2.resize(ad_image, (ad_width, ad_height))",
        "        ",
        "        # Extract position",
        "        x, y = position",
        "        ",
        "        # Ensure the ad fits within the frame",
        "        if x + ad_width > frame.shape[1]:",
        "            x = frame.shape[1] - ad_width",
        "        if y + ad_height > frame.shape[0]:",
        "            y = frame.shape[0] - ad_height",
        "            ",
        "        # Create a copy of the frame",
        "        result = frame.copy()",
        "        ",
        "        # Check if ad has an alpha channel",
        "        if ad_resized.shape[2] == 4:",
        "            # Extract alpha channel",
        "            alpha = ad_resized[:, :, 3] / 255.0",
        "            alpha = np.expand_dims(alpha, axis=2)",
        "            ",
        "            # Extract RGB channels",
        "            rgb = ad_resized[:, :, :3]",
        "            ",
        "            # Get the region of interest in the frame",
        "            roi = result[y:y+ad_height, x:x+ad_width]",
        "            ",
        "            # Blend the ad with the frame using alpha",
        "            blended = (1.0 - alpha) * roi + alpha * rgb",
        "            ",
        "            # Place the blended image back into the frame",
        "            result[y:y+ad_height, x:x+ad_width] = blended",
        "        else:",
        "            # Simple overlay without alpha blending",
        "            result[y:y+ad_height, x:x+ad_width] = ad_resized",
        "            ",
        "        return result",
        "    ",
        "    def process_video(self, frames, ad_image, output_path=None, display_results=True):",
        "        # Process video frames with ad placement",
        "        results = []",
        "        ",
        "        # Reset state variables",
        "        self.prev_frame = None",
        "        self.prev_fgr = None",
        "        self.prev_pha = None",
        "        self.prev_state = None",
        "        ",
        "        for i, frame in enumerate(tqdm(frames, desc=\"Processing frames\")):",
        "            # Every 10 frames, re-detect people and ad placement",
        "            if i % 10 == 0:",
        "                mask = self.generate_matte(frame)",
        "                ad_position = self.find_ad_placement(frame, mask)",
        "            ",
        "            # Place the ad",
        "            result_frame = self.place_ad(frame, ad_image, ad_position)",
        "            results.append(result_frame)",
        "            ",
        "        # Display results",
        "        if display_results:",
        "            display_comparison(frames, results)",
        "            ",
        "        # Save video if output path is provided",
        "        if output_path:",
        "            height, width = results[0].shape[:2]",
        "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")",
        "            out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))",
        "            ",
        "            for frame in results:",
        "                out.write(frame)",
        "                ",
        "            out.release()",
        "            print(f\"\u2705 Video saved to {output_path}\")",
        "            ",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the CrowdFace pipeline and load the video",
        "pipeline = CrowdFacePipeline()",
        "",
        "# Load video frames",
        "sample_video_path = \"sample_video.mp4\"",
        "frames = load_video(sample_video_path, max_frames=50)  # Limit to 50 frames for faster processing",
        "",
        "# Display sample frames",
        "display_frames(frames, title=\"Original Video Frames\")",
        "",
        "# Create a sample ad with transparency",
        "ad_image = create_sample_ad()",
        "",
        "# Display the ad image",
        "plt.figure(figsize=(5, 3))",
        "plt.imshow(cv2.cvtColor(ad_image, cv2.COLOR_BGRA2RGBA))",
        "plt.title(\"Advertisement Image\")",
        "plt.axis('off')",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the video with ad placement",
        "output_path = \"output_video.mp4\"",
        "processed_frames = pipeline.process_video(",
        "    frames, ",
        "    ad_image, ",
        "    output_path=output_path,",
        "    display_results=True",
        ")",
        "",
        "# Display the output video",
        "from IPython.display import Video",
        "",
        "# Display the video",
        "display(Video(output_path, width=800))",
        "",
        "# Provide download option",
        "try:",
        "    from google.colab import files",
        "    files.download(output_path)",
        "    print(f\"\u2705 Download initiated for {output_path}\")",
        "except ImportError:",
        "    print(f\"\u2705 Video saved locally at {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion",
        "",
        "In this notebook, we've demonstrated the complete CrowdFace system, which:",
        "",
        "1. **Segments people in video** using SAM2 (Segment Anything Model 2)",
        "2. **Creates alpha mattes** using RVM (Robust Video Matting)",
        "3. **Places advertisements** in appropriate locations based on segmentation",
        "4. **Processes videos** frame by frame with proper blending",
        "",
        "The CrowdFace system is designed to be robust, with fallback mechanisms when models aren't available. This makes it practical for real-world use cases where not all advanced models might be accessible.",
        "",
        "### Key Features",
        "",
        "- **Automatic Segmentation**: Identifies people in crowd scenes",
        "- **Robust Matting**: Creates high-quality alpha mattes for seamless integration",
        "- **Intelligent Ad Placement**: Places ads in appropriate spaces within the video frame",
        "- **Transparency Support**: Properly handles alpha channels for realistic blending",
        "",
        "### Next Steps",
        "",
        "For a more advanced implementation, consider:",
        "- Adding BAGEL for scene understanding and context-aware ad placement",
        "- Implementing tracking to maintain consistent ad placement",
        "- Optimizing for real-time processing with GPU acceleration"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}